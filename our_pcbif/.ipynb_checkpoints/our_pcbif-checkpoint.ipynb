{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b2b5c6-29c7-4e50-91f9-2a4ed290c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f91b331-45e6-427b-b188-09d128ab563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PCB_iForest_modules.PCB_EIF import PCB_EIF_Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5d6ac9-3084-4152-9cea-921e5d941322",
   "metadata": {},
   "outputs": [],
   "source": [
    "INF_value = 1519889160.0* 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54fa797-f189-4598-9be0-de3a49af26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcb_param_list = {\n",
    "# PCB-iForest\n",
    "'window_size' : 200,\n",
    "'num_trees' : 100,\n",
    "'extension_level' : 1.0, # in percent of dimension - 1.0 denotes to fully extended version\n",
    "'anomaly_threshold_eif' : 0.5,\n",
    "# NDKSWIN-Parameters\n",
    "'alpha' : 0.01,\n",
    "'n_dimensions' : 1,\n",
    "'n_tested_samples' : 0.1,\n",
    "'stat_size' : 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af1da96-43d0-4718-a306-1b6700562811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataFile: str):\n",
    "    '''\n",
    "    return data\n",
    "    @param dataFile: path to the file\n",
    "    @return: data\n",
    "    '''\n",
    "    df = pd.read_csv(dataFile,sep=' ')\n",
    "    class_labels = df.iloc[:, -1:]\n",
    "    final_df = df.astype(float)\n",
    "    data_with_labels = final_df.to_numpy()\n",
    "    return data_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5227d5e4-1eef-42c7-b596-ea2581c3ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(data: np.ndarray):\n",
    "    '''\n",
    "    handling remaining nan, inf values in data instance; separating the class labels from data instance\n",
    "    @param data: numpy array\n",
    "    @return: data, class label\n",
    "    '''\n",
    "    data = data.tolist()\n",
    "    y_label = data[-1]\n",
    "    if isinstance(y_label, str):\n",
    "        if y_label !=  \"Benign\": #anomalous point is represented as 1\n",
    "            y_label = 1\n",
    "\n",
    "        else:\n",
    "            y_label = 0\n",
    "\n",
    "    x_data = np.asarray(data[:-1])\n",
    "    x_data = np.nan_to_num(x_data, nan=0.0, posinf= INF_value,\n",
    "                         neginf=-INF_value)\n",
    "\n",
    "    return x_data, y_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3720112-0064-4b26-b61f-12da40c056cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"shuttle\"\n",
    "update_type = \"U1\"   #U1 or U2\n",
    "D0_path = \"./datasets/\"+dataset_name+\"/\"+update_type+\"/D0.csv\"\n",
    "deltaD_path = \"./datasets/\"+dataset_name+\"/\"+update_type+\"/deltaD.csv\"\n",
    "\n",
    "D0 = load_data(D0_path)\n",
    "deltaD = load_data(deltaD_path)\n",
    "D0_size = len(D0)\n",
    "deltaD_size = len(deltaD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9242d0-9b2d-48a8-9bb4-2598f942a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((D0,deltaD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc50a1c-331d-43be-8cf5-b31e7729ed36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45699"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc887646-fc24-4993-8bf6-a5c0f5d4178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45219 480\n"
     ]
    }
   ],
   "source": [
    "print(D0_size, deltaD_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48de9f2f-0424-4d40-9f9c-7e7d1d4326e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0 shape:  (45699, 10)\n",
      "extension:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"D0 shape: \", data.shape)\n",
    "extension = 1.0  # in percent of dimension - 1.0 denotes to fully extended version\n",
    "print(\"extension: \", extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e32aee-448d-402c-87e4-014dcaf17977",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_EIF = 0.5\n",
    "num_dimensions = 1\n",
    "num_samples = 0.1\n",
    "outlier_content_seen =0\n",
    "\n",
    "\n",
    "hyper_parameters = [[128,100]]\n",
    "list_of_mean_F1_scores = []\n",
    "list_of_mean_exec_times = []\n",
    "list_of_mean_AUC = []\n",
    "list_of_feature_scores_PCB_IBFS = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b9eac-df4b-46ed-b6e3-d6e605de25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in hyper_parameters:\n",
    "    dict_key = \"W\" + str(params[0]) + \"_T\" + str(params[1])\n",
    "    list_of_all_F1_scores = []\n",
    "    list_of_all_exec_times = []\n",
    "    list_of_all_AUC = []\n",
    "    print(\"classifier PCBIF processing \"+ dataset_name + \" with hyper_parameters \" + dict_key)\n",
    "\n",
    "    num_iteration = 1\n",
    "    for i in range(num_iteration):\n",
    "        random_state = np.random.RandomState(i)\n",
    "        initial_time = time.time()\n",
    "        predicted_results = []\n",
    "        true_outliers = []\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives  = 0\n",
    "        false_negatives = 0\n",
    "        samples_used = 0\n",
    "        epsilon = 0.00000001  #small +ve value\n",
    "        PCB_iForest = PCB_EIF_Window(dimension=data.shape[1], window_size=pcb_param_list['window_size'],\n",
    "                                                     num_trees=pcb_param_list['num_trees'],\n",
    "                                                     anomaly_threshold=pcb_param_list['anomaly_threshold_eif'],\n",
    "                                                     extension_level=int(\n",
    "                                                         pcb_param_list['extension_level'] * data.shape[1] - 2),  # -2 here since label column is still contained in data\n",
    "                                                     initial_data_size = D0_size,\n",
    "                                                     alpha=pcb_param_list['alpha'],\n",
    "                                                     n_dimensions=pcb_param_list['n_dimensions'],\n",
    "                                                     n_tested_samples=pcb_param_list['n_tested_samples'],\n",
    "                                                     stat_size=pcb_param_list['stat_size'])\n",
    "        \n",
    "        for data_instance in D0:\n",
    "            samples_used += 1\n",
    "            x_instance, y_instance = preprocessing_data(data_instance)\n",
    "            if y_instance == 1:\n",
    "                true_outliers.append(y_instance)\n",
    "            PCB_iForest.run_train_classifier(x_instance)\n",
    "            '''\n",
    "            predicted_value = PCB_iForest.give_prediction()\n",
    "            predicted_results.append(predicted_value)\n",
    "\n",
    "            if y_instance   == 1:\n",
    "                if predicted_value == 1:\n",
    "                    true_positives +=1\n",
    "                else:\n",
    "                    false_negatives +=1\n",
    "            else:\n",
    "                if predicted_value == 1:\n",
    "                    false_positives +=1\n",
    "                else:\n",
    "                    true_negatives +=1\n",
    "\n",
    "        outlier_content_seen = (len(true_outliers)/samples_used) * 100\n",
    "'''\n",
    "        \n",
    "        for data_instance in D0:\n",
    "            samples_used += 1\n",
    "            x_instance, y_instance = preprocessing_data(data_instance)\n",
    "            if y_instance == 1:\n",
    "                true_outliers.append(y_instance)\n",
    "            #predicted_value = PCB_iForest.predict(x_instance)\n",
    "            predicted_value = PCB_iForest.give_prediction(x_instance)\n",
    "            #PCB_iForest.hello()\n",
    "            predicted_results.append(predicted_value)\n",
    "\n",
    "            if y_instance   == 1:\n",
    "                if predicted_value == 1:\n",
    "                    true_positives +=1\n",
    "                else:\n",
    "                    false_negatives +=1\n",
    "            else:\n",
    "                if predicted_value == 1:\n",
    "                    false_positives +=1\n",
    "                else:\n",
    "                    true_negatives +=1\n",
    "\n",
    "        outlier_content_seen = (len(true_outliers)/samples_used) * 100        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Total no. of samples: \",samples_used)\n",
    "        print(\"outliers content percentage in data: \",outlier_content_seen)\n",
    "        print(\"true_positives={},false_positives={},false_negatives={},true_negatives={}\".format(true_positives,false_positives,false_negatives,true_negatives))\n",
    "\n",
    "        try:\n",
    "                            false_positives_rate = false_positives / (false_positives + true_negatives)\n",
    "        except ZeroDivisionError:\n",
    "                            false_positives_rate = epsilon\n",
    "\n",
    "        try:\n",
    "                            false_negatives_rate = false_negatives / (true_positives + false_negatives)\n",
    "        except ZeroDivisionError:\n",
    "                            false_negatives_rate = epsilon\n",
    "\n",
    "        try:\n",
    "                            recall = true_positives / (\n",
    "                                        true_positives + false_negatives)  # Sensitivity (aka recall, True Positive Rate)\n",
    "        except ZeroDivisionError:\n",
    "                            recall = epsilon\n",
    "\n",
    "        try:\n",
    "                            specificity = true_negatives / (false_positives + true_negatives)  # Specificity (aka True Negative Rate)\n",
    "        except ZeroDivisionError:\n",
    "                            specificity = epsilon\n",
    "\n",
    "        try:\n",
    "                            precision = true_positives / (true_positives + false_positives)  # Precision\n",
    "        except ZeroDivisionError:\n",
    "                            precision = epsilon\n",
    "\n",
    "        try:\n",
    "                            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        except ZeroDivisionError:\n",
    "                            f1_score = epsilon\n",
    "\n",
    "        AUC = (1.0 + recall - false_positives_rate) / 2.0\n",
    "\n",
    "        print(\"Prediction_metrics:\\nrecall={},false_positives_rate={},\\nfalse_negatives_rate={},specificity={},\\nf1_score={}, precision={},\\nAUC={}\".format(recall,false_positives_rate,false_negatives_rate,specificity,f1_score,precision,AUC))\n",
    "        final_time = time.time()\n",
    "        diff = final_time - initial_time\n",
    "        print(\"total runtime: \", diff)\n",
    "        print(\"Average runtime per instance: \",diff/samples_used)\n",
    "\n",
    "        list_of_all_F1_scores.append(f1_score)\n",
    "        list_of_all_exec_times.append(diff)\n",
    "        list_of_all_AUC.append(AUC)\n",
    "\n",
    "        \n",
    "\n",
    "    mean_F1_Score = mean(list_of_all_F1_scores)\n",
    "    mean_exec_time = mean(list_of_all_exec_times)\n",
    "    mean_AUC = mean(list_of_all_AUC)\n",
    "    list_of_mean_F1_scores.append({dict_key:mean_F1_Score})\n",
    "    list_of_mean_exec_times.append({dict_key:mean_exec_time})\n",
    "    list_of_mean_AUC.append({dict_key: mean_AUC})\n",
    "\n",
    "    print(\"Hyperparameter: \" + dict_key + \"\\n\")\n",
    "    for score in list_of_all_F1_scores:\n",
    "        print(json.dumps(score))\n",
    "    print(\"Average F1_score: \",json.dumps(mean_F1_Score, indent=2)+ \"\\n\\n\")\n",
    "\n",
    "    for element in list_of_all_AUC:\n",
    "        print(json.dumps(element))\n",
    "    print(\"Average AUC: \",json.dumps(mean_AUC, indent=2) + \"\\n\\n\")\n",
    "\n",
    "    for element in list_of_all_exec_times:\n",
    "        print(json.dumps(element))\n",
    "    print(\"Average runtime: \",json.dumps(mean_exec_time,indent=2)+ \"\\n\\n\")\n",
    "    print(\"========================================================================================\" + \"\\n\")\n",
    "\n",
    "\n",
    "    with open(result_filename,\"a\") as f:\n",
    "        f.write(\"*************************************%s mean evaluation results***********************************\\n\\n\" %model)\n",
    "        f.write(\"\\nnum of runs: %s\" % num_iteration)\n",
    "        f.write(\"\\noutliers content percentage in data: %s \" % outlier_content_seen)\n",
    "        f.write(\"\\nNDKSWIN parameters: No. of dimensions: %s, No. of samples tested: %s\" % (num_dimensions,num_samples))\n",
    "        f.write(\"\\nModel name: %s\" % model)\n",
    "        if model == \"PCB_EIF\":\n",
    "            f.write(\"\\nextension level : %s\" % extension)\n",
    "            f.write(\"\\nanomaly threshold: %s \"% threshold_EIF)\n",
    "        else:\n",
    "            f.write(\"\\nanomaly threshold: %s \" % threshold_IBFS)\n",
    "        f.write(\"\\nDataset: %s \"% dataFile)\n",
    "        f.write(\"\\ndata shape: %s\" % str(data.shape))\n",
    "        f.write(\"\\n\\nHyperparameter : mean_F1_score\")\n",
    "        for score in list_of_mean_F1_scores:\n",
    "            f.write(json.dumps(score,sort_keys=True,indent=2) + \"\\n\")\n",
    "\n",
    "        f.write(\"Hyperparameter : mean_AUC\")\n",
    "        for element in list_of_mean_AUC:\n",
    "            f.write(json.dumps(element, sort_keys=True, indent=2) + \"\\n\")\n",
    "\n",
    "        f.write(\"Hyperparameter : mean_exec_time\")\n",
    "        for element in list_of_mean_exec_times:\n",
    "            f.write(json.dumps(element, sort_keys=True, indent=2) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db8942c2-6f38-4568-a518-0747080c6782",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18944/3832242952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4a82e-d6ce-4b4e-908b-1c6f9a0eaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier PCBIF processing Shuttle.csv with hyper_parameters W128_T100\n",
    "\n",
    "\n",
    "Total no. of samples:  20000\n",
    "outliers content percentage in data:  7.095\n",
    "true_positives=1382,false_positives=362,false_negatives=37,true_negatives=18219\n",
    "Prediction_metrics:\n",
    "recall=0.9739252995066948,false_positives_rate=0.01948226683170981,\n",
    "false_negatives_rate=0.026074700493305146,specificity=0.9805177331682902,\n",
    "f1_score=0.8738539361365794, precision=0.7924311926605505,\n",
    "AUC=0.9772215163374925\n",
    "total runtime:  35.47285771369934\n",
    "Average runtime per instance:  0.001773642885684967\n",
    "Hyperparameter: W128_T100\n",
    "\n",
    "0.8738539361365794\n",
    "Average F1_score:  0.8738539361365794\n",
    "\n",
    "\n",
    "0.9772215163374925\n",
    "Average AUC:  0.9772215163374925\n",
    "\n",
    "\n",
    "35.47285771369934\n",
    "Average runtime:  35.47285771369934"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
