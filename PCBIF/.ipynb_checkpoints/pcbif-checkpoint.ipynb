{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dd07dd7-629a-43f1-9f52-33a5d22e3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import json\n",
    "from statistics import mean\n",
    "from PCB_iForest_modules.PCB_EIF import *\n",
    "\n",
    "\n",
    "INF_value = 1519889160.0* 100000\n",
    "\n",
    "pcb_param_list = {\n",
    "# PCB-iForest\n",
    "'extension_level' : 1.0, # in percent of dimension - 1.0 denotes to fully extended version\n",
    "'anomaly_threshold_eif' : 0.6,\n",
    "# NDKSWIN-Parameters\n",
    "'alpha' : 0.01,\n",
    "'n_dimensions' : 1,\n",
    "'n_tested_samples' : 0.1,\n",
    "'stat_size' : 30\n",
    "}\n",
    "\n",
    "def load_data(dataFile: str):\n",
    "    '''\n",
    "    return data\n",
    "    @param dataFile: path to the file\n",
    "    @return: data\n",
    "    '''\n",
    "    df = pd.read_csv(dataFile,sep=' ')\n",
    "    class_labels = df.iloc[:, -1:]\n",
    "    final_df = df.astype(float)\n",
    "    data_with_labels = final_df.to_numpy()\n",
    "    return data_with_labels\n",
    "\n",
    "\n",
    "def preprocessing_data(data: np.ndarray):\n",
    "    '''\n",
    "    handling remaining nan, inf values in data instance; separating the class labels from data instance\n",
    "    @param data: numpy array\n",
    "    @return: data, class label\n",
    "    '''\n",
    "    data = data.tolist()\n",
    "    y_label = data[-1]\n",
    "    if isinstance(y_label, str):\n",
    "        if y_label !=  \"Benign\": #anomalous point is represented as 1\n",
    "            y_label = 1\n",
    "\n",
    "        else:\n",
    "            y_label = 0\n",
    "\n",
    "    x_data = np.asarray(data[:-1])\n",
    "    x_data = np.nan_to_num(x_data, nan=0.0, posinf= INF_value,\n",
    "                         neginf=-INF_value)\n",
    "\n",
    "    return x_data, y_label\n",
    "\n",
    "\n",
    "def AAR_F1_AUC(df: pd.DataFrame):\n",
    "    #df.drop(0,axis=1,inplace=True)\n",
    "    df.sort_values(by=0,ascending=False,inplace=True)\n",
    "    df=df.reset_index(drop=True)\n",
    "    AAR = np.mean(df.index[df[1] == 1].tolist())\n",
    "    \n",
    "    num_anomalies=df[1].value_counts()[1]\n",
    "    \n",
    "    pred_labels = np.zeros(len(df))\n",
    "    pred_labels[:num_anomalies]=1\n",
    "    df['pred_labels']=pred_labels\n",
    "    \n",
    "    \n",
    "    f_score=metrics.f1_score(df[1],df['pred_labels'])\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df[1], df[0], pos_label=1)\n",
    "    \n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    print(AAR,f_score,auc_score,end=' ')\n",
    "\n",
    "\n",
    "def recall_AUC(scores_labels:pd.DataFrame):\n",
    "    #sort the anomlayscores in descending order\n",
    "    scores_labels.sort_values(by=0,ascending=False,inplace=True)\n",
    "    total_anomalies=scores_labels[1].value_counts()[1]\n",
    "    #add predicted labels column in dataframe------\n",
    "    pred_labels = np.zeros(len(scores_labels))\n",
    "    pred_labels[:total_anomalies]=1\n",
    "    scores_labels['pred_labels']=pred_labels\n",
    "    for i in [total_anomalies, int(len(scores_labels)*0.1), int(len(scores_labels)*0.2), int(len(scores_labels)*0.3),]:\n",
    "        head_rows=scores_labels.head(i)\n",
    "        #head_rows=scores_labels.head(int(len(scores_labels)/2))\n",
    "        tp_Di=len(head_rows[(head_rows[1] == 1) & (head_rows.index < Di_size)])\n",
    "        tp_deltaD=len(head_rows[(head_rows[1] == 1) & (head_rows.index >= Di_size)])\n",
    "        tp_full=len(head_rows[(head_rows[1] == 1)])\n",
    "        recall_full=tp_full/total_anomalies\n",
    "        recall_Di = tp_Di/Di_anomalies\n",
    "        recall_deltaD='NAN'\n",
    "        if update_type=='CDI':\n",
    "            recall_deltaD = tp_deltaD/deltaD_anomalies\n",
    "        print(recall_Di,recall_deltaD,recall_full,end=' ')\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(scores_labels[1], scores_labels[0], pos_label=1)\n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    print(auc_score)\n",
    "\n",
    "dataset_name = 'gassensor'\n",
    "update_type = 'CDIII'   # CDI or CDII or CDIII\n",
    "numiTrees = 100\n",
    "num_iteration = 1\n",
    "'''\n",
    "dataset_name = str(sys.argv[1])\n",
    "update_type = str(sys.argv[2])   # CDI or CDII or CDIII\n",
    "numiTrees = int(sys.argv[3])\n",
    "num_iteration = int(sys.argv[4])\n",
    "'''\n",
    "\n",
    "Di_path = \"../io_directory/Di.csv\"\n",
    "deltaD_path = \"../io_directory/deltaD_\"+update_type+\".csv\"\n",
    "\n",
    "\n",
    "Di = load_data(Di_path)\n",
    "deltaD = load_data(deltaD_path)\n",
    "Di_size = len(Di)\n",
    "deltaD_size = len(deltaD)\n",
    "\n",
    "\n",
    "data = np.concatenate((Di,deltaD))\n",
    "sample_size = min(deltaD_size,256)\n",
    "\n",
    "\n",
    "dict_key = \"W\" + str(sample_size) + \"_T\" + str(numiTrees)\n",
    "#print(\"extension_level=\"+str(int(pcb_param_list['extension_level'] * data.shape[1] - 2)))\n",
    "\n",
    "deltaD_labels=pd.read_csv('../io_directory/deltaD_'+update_type+'_labels.csv',sep=' ',header=None)\n",
    "Di_labels=pd.read_csv('../io_directory/Di_'+update_type+'_labels.csv',sep=' ',header=None)\n",
    "\n",
    "\n",
    "Di_anomalies=len(Di_labels[(Di_labels[0] == 1)])\n",
    "deltaD_anomalies=len(deltaD_labels[(deltaD_labels[0] == 1)])\n",
    "total_anomalies=Di_anomalies+deltaD_anomalies\n",
    "\n",
    "\n",
    "avg_Di_score = pd.DataFrame(np.zeros(Di_size),columns=[0])\n",
    "avg_deltaD_score = pd.DataFrame(np.zeros(deltaD_size),columns=[0])  \n",
    "\n",
    "\n",
    "for i in range(num_iteration):\n",
    "    random_state = np.random.RandomState(i)\n",
    "    initial_time = time.time()\n",
    "    samples_used = 0\n",
    "       \n",
    "        \n",
    "    ix = np.random.choice(Di_size, size=sample_size, replace=False)\n",
    "    Di_sample = Di[ix,:-1]\n",
    "        \n",
    "        \n",
    "    F = EiForest(Di_sample, ntrees=numiTrees, sample_size=sample_size,\n",
    "                                         threshold=pcb_param_list['anomaly_threshold_eif'], ExtensionLevel=int(\n",
    "                                         pcb_param_list['extension_level'] * data.shape[1] - 2)) \n",
    "    #print('model initiated')\n",
    "    \n",
    "    F.ensemble_fit(Di_sample)  # EiForest model trained\n",
    "    ndkswindow = ndk(alpha=pcb_param_list['alpha'], \n",
    "                         data=Di_sample,\n",
    "                         n_dimensions=pcb_param_list['n_dimensions'],\n",
    "                         window_size=sample_size,\n",
    "                         stat_size=pcb_param_list['stat_size'],\n",
    "                         n_tested_samples=pcb_param_list['n_tested_samples'],\n",
    "                         fixed_checked_dimension=False, \n",
    "                         fixed_checked_sample=False)\n",
    "                    \n",
    "        \n",
    "    PCBIF_time = time.time()\n",
    "        \n",
    "    ix = np.random.choice(deltaD_size, size=sample_size, replace=False)\n",
    "    deltaD_sample = deltaD[ix,:-1]\n",
    "       \n",
    "    for data_instance in deltaD[:,:-1]:\n",
    "        overall_anomaly_score, individual_anomaly_score = F.compute_paths(data_instance) # self.window[-1]\n",
    "        F.compare_scores(overall_anomaly_score, individual_anomaly_score)\n",
    "        #prediction = F.predict_function(overall_anomaly_score)\n",
    "            \n",
    "    ndkswindow.add_element(deltaD_sample)\n",
    "    if ndkswindow.detected_change():\n",
    "        print(\"modelupdates\",end=' ')\n",
    "        F.update_model(deltaD_sample.tolist())\n",
    "    else:\n",
    "        print('',end=' ')   \n",
    "    PCBIF_time = time.time()- PCBIF_time\n",
    "\n",
    "    Di_ascore = pd.DataFrame(columns=[0,1])\n",
    "    for data_instance in Di:\n",
    "        x_instance, y_instance = preprocessing_data(data_instance)\n",
    "        overall_anomaly_score, individual_anomaly_score = F.compute_paths(x_instance) # self.window[-1]\n",
    "        Di_ascore.loc[len(Di_ascore.index)] = [overall_anomaly_score,Di_labels[0].iloc[len(Di_ascore.index)]]  \n",
    "        #Di_ascore.to_csv('../io_directory/PCBIF_AS_Di.csv',sep=' ',index=False, header=None)\n",
    "    \n",
    "    avg_Di_score[0]+=Di_ascore[0]\t\n",
    "\n",
    "\n",
    "\n",
    "    deltaD_ascore = pd.DataFrame(columns=[0,1])\n",
    "    for data_instance in deltaD:\n",
    "        x_instance, y_instance = preprocessing_data(data_instance)\n",
    "        overall_anomaly_score, individual_anomaly_score = F.compute_paths(x_instance) # self.window[-1]\n",
    "        deltaD_ascore.loc[len(deltaD_ascore.index)] = [overall_anomaly_score,deltaD_labels[0].iloc[len(deltaD_ascore.index)]]\n",
    "        #deltaD_ascore.to_csv('../io_directory/PCBIF_AS_deltaD.csv',sep=' ',index=False, header=None)\n",
    "    \n",
    "    avg_deltaD_score[0]+=deltaD_ascore[0]\t\n",
    "         \n",
    "    \n",
    "    print(PCBIF_time,end=' ')\n",
    "    \n",
    "    recall_AUC(pd.concat([Di_ascore,deltaD_ascore]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51749769-d745-400c-b84b-b1945635e863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
